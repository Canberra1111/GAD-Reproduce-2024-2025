{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4899f9-181b-48be-832c-39f9c3556478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "from GCN import GCN\n",
    "from GraphSAGE import GraphSAGE\n",
    "from dataset import Dataset\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, roc_auc_score, precision_score, confusion_matrix,precision_recall_curve,auc\n",
    "from BWGNN import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ddpm.feature_train import ddpmFeatures\n",
    "from util import saveOneStep\n",
    "\n",
    "def getChangeNode(H_ALL,C,change_ratio1,change_ratio2):\n",
    "    np.random.shuffle(C)\n",
    "    C1 = C[:int(change_ratio1*len(C))]\n",
    "    # H_ALL.remove(C1)\n",
    "    H_ALL = H_ALL[:int(change_ratio2*len(H_ALL))]\n",
    "    return C1,H_ALL\n",
    "\n",
    "def train(model, g, args):\n",
    "    # logging.basicConfig(filename=f\"./log/logger_\"+args.dataset+\".log\",filemode=\"a\",format=\"%(asctime)s-%(name)s-%(levelname)s-%(message)s\",level=logging.INFO)\n",
    "    # logger=logging.getLogger('BWGNN')\n",
    "    features = g.ndata['feature'].clone()\n",
    "    homo = args.homo\n",
    "    labels = g.ndata['label']\n",
    "    abchr = args.abchr\n",
    "    nchr = args.nchr\n",
    "    neighborchr = args.neighborchr\n",
    "    dataset_name = args.dataset\n",
    "    normal_idx=np.where(labels!=1)[0]\n",
    "    abnormal_idx=np.where(labels==1)[0]\n",
    "    index = list(range(len(labels)//3))\n",
    "    if dataset_name == 'amazon':\n",
    "        index = list(range(3305, len(labels)//3))\n",
    "    random_state = 2\n",
    "    idx_train, idx_rest, y_train, y_rest = train_test_split(index, labels[index], stratify=labels[index],\n",
    "                                                            train_size=args.train_ratio,\n",
    "                                                            random_state=random_state, shuffle=True)\n",
    "    idx_rest,idx_change,y_rest,y_change = train_test_split(idx_rest, y_rest, stratify=y_rest,\n",
    "                                                            test_size=0.5,\n",
    "                                                            random_state=random_state, shuffle=True)\n",
    "    idx_valid, idx_test, y_valid, y_test = train_test_split(idx_rest, y_rest, stratify=y_rest,\n",
    "                                                            test_size=0.67,\n",
    "                                                            random_state=random_state, shuffle=True)\n",
    "    # t = list(abnormal_idx)\n",
    "    # temp = t[:len(t)//3]\n",
    "    # saveOneStep(graph,temp,labels)\n",
    "    abC = []\n",
    "    AB = list(abnormal_idx) +idx_train\n",
    "    for i in AB:\n",
    "        if i in idx_train and i in list(abnormal_idx):\n",
    "            if i not in abC:\n",
    "                abC.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    nC = []\n",
    "    AB = list(normal_idx) +idx_train\n",
    "    for i in AB:\n",
    "        if i in idx_train and i in list(normal_idx):\n",
    "            if i not in nC:\n",
    "                nC.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    '''\n",
    "    obtain replaced nodes\n",
    "    '''\n",
    "\n",
    "    abC1,abH_ALL = getChangeNode(idx_change,abC,abchr,neighborchr)\n",
    "    nC1,nH_ALL = getChangeNode(idx_change,idx_test+idx_valid,nchr,neighborchr)\n",
    "    ab_features = features[abC]\n",
    "    \n",
    "    non_features = features[nC][:len(abC)]\n",
    "    # put ab_features to generate new gen_features\n",
    "    gen_features = ddpmFeatures(ab_features,non_features)\n",
    "    # gen_features = ab_features\n",
    "    # features of anomalies\n",
    "    abC2 = [ i+len(labels)//3 for i in abC1]\n",
    "    nC2 = [ i+2*len(labels)//3 for i in nC1]\n",
    "    for i in abH_ALL:\n",
    "        t = random.randint(0,len(abC)-1)\n",
    "        features[i+len(labels)//3] =gen_features[t]\n",
    "    for i in nH_ALL:\n",
    "        t = random.randint(0,len(abC)-1)\n",
    "        features[i+2*len(labels)//3] =gen_features[t]\n",
    "\n",
    "    C1 = nC1\n",
    "    C2 = nC2\n",
    "    train_mask = torch.zeros([len(labels)]).bool()\n",
    "    val_mask = torch.zeros([len(labels)]).bool()\n",
    "    test_mask = torch.zeros([len(labels)]).bool()\n",
    "    train_mask[idx_train+abC2] = 1\n",
    "    val_mask[idx_valid] = 1\n",
    "    test_mask[idx_test] = 1\n",
    "    print('train/dev/test samples: ', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    best_f1, final_tf1, final_trec, final_tpre, final_tmf1, final_tauc,finanl_tauc_pr = 0., 0., 0., 0., 0., 0.,0.\n",
    "\n",
    "    weight = (1-labels[train_mask]).sum().item() / labels[train_mask].sum().item()\n",
    "    print('cross entropy weight: ', weight)\n",
    "    time_start = time.time()\n",
    "\n",
    "    for e in range(args.epoch):\n",
    "        model.train()\n",
    "        logits = model(g,features)\n",
    "        logits[C1] = logits[C2]\n",
    "        \n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask], weight=torch.tensor([1., weight]))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "        probs = logits.softmax(1)\n",
    "        f1, thres = get_best_f1(labels[val_mask], probs[val_mask])\n",
    "\n",
    "        preds = numpy.zeros_like(labels)\n",
    "        preds[probs[:, 1] > thres] = 1\n",
    "        trec = recall_score(labels[test_mask], preds[test_mask])\n",
    "        tpre = precision_score(labels[test_mask], preds[test_mask])\n",
    "        tmf1 = f1_score(labels[test_mask], preds[test_mask], average='macro')\n",
    "        tauc = roc_auc_score(labels[test_mask], probs[test_mask][:, 1].detach().numpy())\n",
    "        precision,recall,_ = precision_recall_curve(labels[test_mask], probs[test_mask][:, 1].detach().numpy())\n",
    "        tauc_pr = auc(recall,precision)\n",
    "        if best_f1 < f1:\n",
    "            best_f1 = f1\n",
    "            final_trec = trec\n",
    "            final_tpred = preds\n",
    "            final_tpre = tpre\n",
    "            final_tmf1 = tmf1\n",
    "            final_tauc = tauc\n",
    "            finanl_tauc_pr = tauc_pr\n",
    "        print('Epoch {}, loss: {:.4f}, val mf1: {:.4f}, (best {:.4f})'.format(e, loss, f1, best_f1))\n",
    "\n",
    "    time_end = time.time()\n",
    "    \n",
    "    print('time cost: ', time_end - time_start, 's')\n",
    "    print('Test: REC {:.2f} PRE {:.2f} MF1 {:.2f} AUC {:.2f} AUC_PR {:.2f}'.format(final_trec*100,\n",
    "                                                                     final_tpre*100, final_tmf1*100, final_tauc*100,finanl_tauc_pr*100))\n",
    "    \n",
    "    # logger.info('dataset {:s} {:.2f}'.format(args.dataset,args.train_ratio))\n",
    "    # logger.info('random_state {:d}  Test: REC {:.2f} PRE {:.2f} MF1 {:.2f} AUC {:.2f} AUC_PR {:.2f}'.format(random_state,final_trec*100,\n",
    "                                                                    #  final_tpre*100, final_tmf1*100, final_tauc*100,finanl_tauc_pr*100))\n",
    "    # logger.info('abchr {:.1f}, nchr {:.1f},neighborchr {:.1f}\\n'.format(abchr,nchr,neighborchr))\n",
    "    return final_tmf1, final_tauc\n",
    "\n",
    "\n",
    "# threshold adjusting for best macro f1\n",
    "def get_best_f1(labels, probs):\n",
    "    best_f1, best_thre = 0, 0\n",
    "    for thres in np.linspace(0.05, 0.95, 19):\n",
    "        preds = np.zeros_like(labels)\n",
    "        preds[probs[:,1] > thres] = 1\n",
    "        mf1 = f1_score(labels, preds, average='macro')\n",
    "        if mf1 > best_f1:\n",
    "            best_f1 = mf1\n",
    "            best_thre = thres\n",
    "    return best_f1, best_thre\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='BWGNN')\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"yelp\",\n",
    "                        help=\"Dataset for this model (yelp/amazon/tfinance/pubmed)\")\n",
    "    parser.add_argument(\"--train_ratio\", type=float, default=0.01, help=\"Training ratio\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.01, help=\"learning rate\")\n",
    "    parser.add_argument(\"--abchr\", type=float, default=1, help=\"Abnormal data exchange ratio\")\n",
    "    parser.add_argument(\"--nchr\", type=float, default=0, help=\"Normal data exchange ratio\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"BWGNN\",\n",
    "                        help=\"(BWGNN/GCN/GraphSAGE)\")\n",
    "    parser.add_argument(\"--neighborchr\", type=float, default=0.4, help=\"neighbor data exchange ratio\")\n",
    "    parser.add_argument(\"--hid_dim\", type=int, default=64, help=\"Hidden layer dimension\")\n",
    "    parser.add_argument(\"--order\", type=int, default=2, help=\"Order C in Beta Wavelet\")\n",
    "    parser.add_argument(\"--homo\", type=int, default=0, help=\"1 for BWGNN(Homo) and 0 for BWGNN(Hetero)\")\n",
    "    parser.add_argument(\"--epoch\", type=int, default=100, help=\"The max number of epochs\")\n",
    "    parser.add_argument(\"--run\", type=int, default=1, help=\"Running times\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    dataset_name = args.dataset\n",
    "    homo = args.homo\n",
    "    order = args.order\n",
    "    h_feats = args.hid_dim\n",
    "    graph = Dataset(dataset_name, homo).graph\n",
    "    if (homo):\n",
    "        c = list(graph.edges())\n",
    "        h =c[0]+graph.num_nodes()\n",
    "        k=c[1]+graph.num_nodes()\n",
    "        h2 =c[0]+2*graph.num_nodes()\n",
    "        k2=c[1]+2*graph.num_nodes()\n",
    "        c[0] = torch.cat((c[0],h,h2))\n",
    "        c[1] = torch.cat((c[1],k,k2))\n",
    "        c = tuple(c)\n",
    "        graph2= dgl.graph(c)\n",
    "    else:\n",
    "        c1 = list(graph.edges(etype='net_rsr'))\n",
    "        h =c1[0]+graph.num_nodes()\n",
    "        k=c1[1]+graph.num_nodes()\n",
    "        h2 =c1[0]+2*graph.num_nodes()\n",
    "        k2=c1[1]+2*graph.num_nodes()\n",
    "        c1[0] = torch.cat((c1[0],h,h2))\n",
    "        c1[1] = torch.cat((c1[1],k,k2))\n",
    "        c1 = tuple(c1)\n",
    "\n",
    "        c2 = list(graph.edges(etype='net_rtr'))\n",
    "        h =c2[0]+graph.num_nodes()\n",
    "        k=c2[1]+graph.num_nodes()\n",
    "        h2 =c2[0]+2*graph.num_nodes()\n",
    "        k2=c2[1]+2*graph.num_nodes()\n",
    "        c2[0] = torch.cat((c2[0],h,h2))\n",
    "        c2[1] = torch.cat((c2[1],k,k2))\n",
    "        c2 = tuple(c2)\n",
    "\n",
    "        c3 = list(graph.edges(etype='net_rur'))\n",
    "        h =c3[0]+graph.num_nodes()\n",
    "        k=c3[1]+graph.num_nodes()\n",
    "        h2 =c3[0]+2*graph.num_nodes()\n",
    "        k2=c3[1]+2*graph.num_nodes()\n",
    "        c3[0] = torch.cat((c3[0],h,h2))\n",
    "        c3[1] = torch.cat((c3[1],k,k2))\n",
    "        c3 = tuple(c3)\n",
    "\n",
    "        print(graph)\n",
    "        graph_data = {\n",
    "            ('review', 'net_rsr', 'review'):c1,\n",
    "            ('review', 'net_rtr', 'review'):c2,\n",
    "            ('review', 'net_rur', 'review'):c3\n",
    "        }\n",
    "        graph2 = dgl.heterograph(graph_data)\n",
    "\n",
    "    graph2.ndata['feature'] = torch.cat((graph.ndata['feature'],graph.ndata['feature'],graph.ndata['feature']))\n",
    "    graph2.ndata['label'] = torch.cat((graph.ndata['label'],graph.ndata['label'],graph.ndata['label']))\n",
    "\n",
    "    f = graph2.ndata['label'] \n",
    "    graph = graph2\n",
    "    in_feats = graph.ndata['feature'].shape[1]\n",
    "\n",
    "    num_classes = 2\n",
    "\n",
    "    if args.run == 1:\n",
    "\n",
    "        if homo:\n",
    "            if args.model =='BWGNN':\n",
    "                model = BWGNN(in_feats, h_feats, num_classes, graph, d=order)\n",
    "            elif args.model =='GCN': \n",
    "                model = GCN(None,in_feats,h_feats,num_classes,3,F.relu,0.5)\n",
    "            elif args.model =='GraphSAGE':  \n",
    "                model = GraphSAGE(None,in_feats,h_feats,num_classes,3,F.relu,0.5,aggregator_type='pool')\n",
    "        else:\n",
    "            model = BWGNN_Hetero(in_feats, h_feats, num_classes, graph, d=order)\n",
    "        train(model, graph, args)\n",
    "\n",
    "    else:\n",
    "        final_mf1s, final_aucs = [], []\n",
    "        for tt in range(args.run):\n",
    "            if homo:\n",
    "                if args.model =='BWGNN':\n",
    "                    model = BWGNN(in_feats, h_feats, num_classes, graph, d=order)\n",
    "                elif args.model =='GCN': \n",
    "                    model = GCN(None,in_feats,h_feats,num_classes,3,F.relu,0.5)\n",
    "                elif args.model =='GraphSAGE':  \n",
    "                    model = GraphSAGE(None,in_feats,h_feats,num_classes,3,F.relu,0.5,aggregator_type='pool')\n",
    "            else:\n",
    "                model = BWGNN_Hetero(in_feats, h_feats, num_classes, graph, d=order)\n",
    "            mf1, auc1 = train(model, graph, args)\n",
    "            final_mf1s.append(mf1)\n",
    "            final_aucs.append(auc1)\n",
    "        final_mf1s = np.array(final_mf1s)\n",
    "        final_aucs = np.array(final_aucs)\n",
    "        print('MF1-mean: {:.2f}, MF1-std: {:.2f}, AUC-mean: {:.2f}, AUC-std: {:.2f}'.format(100 * np.mean(final_mf1s),\n",
    "                                                                                            100 * np.std(final_mf1s),\n",
    "                                                               100 * np.mean(final_aucs), 100 * np.std(final_aucs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
